Ensemble technique:
	Decision taken using wisdom of a crowd
	voting taken from multiple models in terms of decisions which together determine the result.
	if individual decisions are taken then it is singular technique with all the trust in one basket. However in ensemble method we are building hundreds of models and take average of those models towards making final decision.
	key strength of the technique is to have every model we build to be independent of each other. And the assumption is decisions made by each model is not corelated with each other or influenced by each other.
	several models across decision tree of ensemble method is also called base estimator.
	Class with most votes becomes models prediction.
	A large number of relatively uncorrelated models (trees) will outperform any of the individual models.
	Bootstraping and choosing smaller number of variables as splitting criteria for the decision tree will have to ensure models are uncorrelated to each other.
	Bootstraping is also referred to as bagging.  bagging introduces cross validation technique.

Random forest: Forest of trees.
	CART model is what is used by default in order to construct multitude of models.
	In this we create multiple decision trees. models for each tree are different and independent of each other.
	There should be variation among decision trees when it comes to its data and parameters.

	Boot strapping helps in setting up the randomness across the trees.
		drawing multiple random sample with replacements of data is called boot strapping  and uses random set of independent variables at each split to make multiple decision tree with reduced variables on the bootstrapped data sets.
		we combine the predictions across the tree to obtain the final decision.
		we use voting principle in case of classification model (for categorical variable) for prediction and we perform averaging for continous prediction	
		Original Data set shall be sourced in such a way to populate multiple trees with unique combinations of data including potential duplication of same rows from original dataset
	 	Columns can also be randomly sampled from the original dataset across the decision trees. i.e smaller number of columns are used for splitting criteria. Decision tree uses gini index and gini gain to pick the best independent variable to split the data at each node
		Before the split happens the columns are randomly selected which are further validated to narrow down the best independent variable.
		choosing the smaller number of independent variables reduces the probability of corelation compared to using larger number of independent variables. However this reduction reduces the strength of the prediction.
		We need to find an optimal range of number of independent variables to maintain the balance between prediction strength vs corelation.
	Properties of each decision tree (desirable):
		prediction strength of each individual tree must be high
		decision tree must not be corelated to each other.

	Out of bag data points: (used to measure accuracy of the random forest)
		Sample records not present in the boot strapped datasets compared to the original data set is called out of the bag. we use out of bag to make predictions.
		Every decision tree will undergo prediction for the out of bag records with respect to their corresponding boot strapped data set.
			Ultimately an error value is calculated by random forest towards how well all the models has performed for all the out of bag records in being able to predict for them.
			proportion of out of bag samples incorrectly classified is called out of bag error.
		Approximately 1/3rd of the original dataset does not end up in the bootstrapped datasets and ends up in out of bag.

		Probability of 1 row getting sampled from the original dataset into the boot strapped dataset is 1/n
		probability of 1 row not getting sampled from the original dataset hence = 1-1/n
		probability of n row not getting sampled from the original dataset hence = (1-1/n)^n
		When n becomes very large (infinite) then 
			probability of n row not getting sampled from the original dataset =1/e =37%(e is exponentential)
		Hence out of bag data points = 37% and the remaining 63% is what we call as in-sample records ending up in each of the bootstrapped dataset uniquely.

cross validation: is done only on the training data set and 1 part of the n split of training data set will be test data set.
	each split of data get to be part of training set for n-1 times and once as test data set.
	training set gets to vote for respective  out of bag entries only. meaning every data split will be processed for voting from other splits accordingly to derive out of bag error. this is repeated n times for each combo of train and test set for the training data.
	think of multiple bags that cross validation creates in terms of n scenarios with each out of the n bags becoming a test data set.
	industry standard uses cross validation=10 during the gridsearchCV.

overfitment: works well with training data and not test data
underfitment: opposite of overfitment.

In random forest, for number of features we can use square root of number of columns for large dataset or n/2 as a standard approach for boot strapping process.
Random forest is as such is not pruned but we dont prune it.

Programming:
All object data types needs to be converted to integers by converting 
while building RandomForestClassifier, we can control how many trees should be created (number of estimator) and an optional oob_score(True/False).
Hyper parameters for decisions trees: Maximum depth, minimum number of samples per leaf, maximum number of feature
max features number of columns are chosen randomly for comparing gini gain to decide on the split. the maximum split can be number of estimator * maximum depth
oob score determines the accuracy of the model reflecting accuracy of the prediction on the out of bag entries across the decision trees. 

GridsearchCV module allows narrowing down the hyper parameter values by taking the dictionary of different values for each of the parameter to recommend the best fit in it.
GridSearchCV takes RandomForestClassifer and the dictionary of parameters as inputs and evaluate all the combinations in the dictionary and returns best_params_. total combinations will be product of number of listed values across hyper parameters.
grid_search's best_estimator_ component will have the proper parameters configured for RandomForestClassifier on which we can apply predicted value
based on the best estimator we can either predict class or probability for the training and testing set accordingly.

Using the predicted classes and the corressponding training and testing labels/classes we can create confusion matrix and classification report.
confusion matrix: predicts negatives in column 0 and predicats positives in column 1, actual negative in row 0 and actual positive in row 1
classification report provides precision, recall, f1 score and the supported entries from the data appropriately.

Using the predicted probabilities for the targeted classes and the corresponding training/testing labels we can compute AUC score and plot ROC curve.
ROC curve contains false positive rate in x axis and true postive rate in the y axis. we get the FPR, TPR and the threshold through roc_curve function.

confusion matrix compares trained labels versus predicted lables for the trainning data set. 
imputing outliers and its approach depends on the domain expertise. Also there could be situation that train/test data has outlier situations that may not be production scenario and hence calling for an outlier treatement during model building.

best param is narrowed down with trial and error method for various values of hyper parameters during grid search. this step is called model tuning.
model converges well if train and test deviate within +/- 10%  i.e recall rate of 0.04 for train and 0.02 for test means the model converges well. whereas 0.10 and 0.25 is not.
finally feature importance will be used to write the insight.

please note if there are imbalance in the data set meaning % of the data for class 1 is lets say 15% whereas for class 0 is 85%. we need to fix this to get the model to work or even relook at the model selection.
Also when training data set reports better classification report with highest values for precision, recall and f1 score whereas test data did not it is a sign of overfitted model. In other words classification report should return values close to each other across train and test datasets.

INDUSTRY TREND OF CHOOSING ML ALGORITHMS WITHIN THEIR FIRMS:
PLEASE NOTE THAT EVERY FIRM MIGHT HAVE THEIR OWN PRE DECIDED SET OF MODELS TO BE USED BASED ON VARIOUS CONTEXT OR SCENARIOS. THIS CAN BE ASSUMED AS REFERENE ARCHITECTURE EQUIVALENT OF DATA SCIENCE.