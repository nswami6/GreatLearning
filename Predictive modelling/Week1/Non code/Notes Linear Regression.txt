summary:
Structure + Technique + Learning process(Gradient descent method to minimize sum of squared errors)
OLS vs Gradient descent
Validation (hypothesis testing)
Evaluating independent variables and its contribution to the model.
Model inaccuracy measures: Bias, variance and random error
Model evaluation (Co efficient of determinant)

Detail
Linear regression is used to find relationship between two continous variables.
OLS (Ordinary least square)- technique used to create linear regression. it tries to reduce the mean sum of squared error (MSSE)
Learning process - gradient descent methodology

Multivariate linear regression model = Structure of linear regression model+ OLS +Gradient descent methodology

is the multivariate linear regression model worthy enough to productionize?
is this model only going to work with training data set or does it represent the way things work in the universe?
above questions can be answered using hypothesis testing. in this, alternate hypothesis is the linear regression model.

assumptions we make on the model when gets diluted might affect the production performance of the model. these assumptions could also be weakness of the model which 
		might not be met in the real world in certain cases.

need to understand advantages and disadvantages of the model towards its risk mitigation.

every model we build it represents the real world process that could be tangible (prediction of miles per gallon of a car- cars behaviour prediction is tangible 
			process) or intangible (i.e stock exchange, customer churn prediction etc)

what is process?  Output = process (X).  x is independent/predictor variables.
	in math we refer to those process as functions y=F(X). y is target/predicted variables/dependent variable.
	function's mathematics is called model. which in turn represents a process in the real world.
	to build model we need data that is generated by the process and this data is called sample which is obtained from all the data that is generated by the 
			existing process in the real world and it is called universe.
	we build model on sample assuming it represents process in the universe.
	differences between sample and universe: 
		A universe is always under flux constantly changing, whereas sample is static i.e predicting time taken by taxi to reach point B from point A where 
			we use past trips by itinerary.  this data is static data on which we build the model wherein in the real world some construction activity 
			might affect the traffic that will not be reflected by prediction model. this is flux in universe.
			Flux needs to be accounted for in the model to make it work in the universe. 
			also sample reliability needs to established as representation of todays universe to ensure better scores on the model prediction.
	difference between prediction and actual is called error. In other words error is a distance between the point and the regression line.
		 a models prediction will not be accurate. bias, variance and random errors occur.
		we need to understand source of these errors and mitigate/minimize these errors to get a right fit, in other words a generalized model.
		right fit model minimizes bias and variance. we dont have control over random errors.
		opposite of right fit is underfit or overfit.
	process, sample, universe, errors are introduction to get into discussion of linear regression model.

Linear regression
	all models express relationship between X and Y in different ways. 
		decision tree express through decision rules
		naive bayes expresses through likelihood ratios between X and Y and prior and posterior
		linear regression does it through mathematical equation. and predicts a running real numbers which is where term regression comes from. 
			the term linear represents relation ship of X and Y as linear addition or summation /combination of input (i.e explanatory or independent 
			variables. i.e y=mx+c  or y=m1x1 + m2x2 + ... + c.  where m is coefficient or weights  or slope and c is constant or intercept or bias.
			since m is raised to the power of 1 we call it linear regression model.
		hence y is a summation of weighted input variables and constant.
		in a 2 dimension, linear regression could be a line, in 3 dimension it is a plane and anything above 3 dimensions it will be an hyper plane.  

	Note: In Scatter plot we can see for one x there are many y's because there are other variables that are impacting the y resulting in the scatter. 
		however we need to understand the trend in the plot and represent this trend in the form of a model.


Hypothesis testing: 
	Null should be acceptable. compare central values of sample and universe towards this.
	if Null is not accetable then the reliability of dataset is questionable.

Evaluate independent variables on their influence on target variables... i.e correlation. Pearson's correlation is used by default represented by 'r' 
												(ranges between -1 and +1)
correlation= covariance expressed in terms of standard deviation.  variables with r values closer to or equal to 0 cannot be used in linear regression model. if r=0 
		distribution looks like a cloud.
we use corelation to understand importance of variables to be used in the model.

Best fit line:
	there can be infinite number of lines we can draw for relationship between x and y but the line that captures the relationship most accurately is called best 
										fit line.
	Gradient descent method is one way and a mathematical way to find the best fit line among all the lines. this method finds the best fit line more quickly than
									any other methods.
	Gradient descent method uses partial derivatives on the parameters to minimize sum of squared errors.
	y is the value/label given in the dataset, we predict y hat. y - y hat is the error..
	metric that we use to determine the best fit line is called average sum of squared errors (from above formalae i.e (y-y hat)^2 ) across all possible lines.
	dividing avg sum of squared errors by number of observations gives mean sum of squres error (MSSE). this is found using gradient descent method. 
		The line with least MSSE is the best fit line.
	best fit line goes through the point where x bar and y bar meets.
	Linear regression fits a line to the data by finding the regression coefficient that results in the smallest MSE.

co efficient of determinent = r^2 which is not equal to r * r (i.e corelation) when it comes to cases beyond 1 input and 1 output variable
	this metric is used to evaluate models. this is calculated based on  all data points put together in the best fit line case.
	co eff of determinent ranges between 0 and 1 with 1 being the best case.
	y bar - y hat is called total error (SST) out of which model can capture difference between expected and actual which is called regression error (SSR will be 
							sum of squares of regression error).
	y bar - actual y is unexplained by the model. this leads to SSE.
	of the total variance, how much has been explained by the model is called co efficient of determitent(r^2)
	SST = SSE + SSR

r is used to build the model by comparing attributes. ranges from -1 to +1 
r^2 is used to evaluate models. ranges from 0 to 1. Higher the r^2 better the model is. r=SSR/SST being close to 0 is a poor fit model.


OLS is not a scalable technique when it comes to increasing independent variables or increasing data points. Gradient descent is alternate method to arrive at 
											linear regression.

m and c are learnt from the data and error is learnt from it. magnitude of error can be controlled by manipulating m and c.
E = F(m,c) = (y- y hat)^2 .
Error function takes a parabolic shape which will have an absolute minimum with close to 0 zero error but not absolute 0.
	 any combo of m and c will give an error in a model. The combo (m optimal, c optimal) that takes to absolute minima will give the best fit line.

Initially the m and c is generated at random to predict y hat. Using gradient descent along with concept of partial derivativs the gradient descent algorithm finds
				optimal m and c giving least sum of squared errors.
Usually gradient descent are discussed along with the concept of contours that are not part of the current syllabus.	


Hypothesis testing:
Null hypothesis states that independant variables have no co relationship with target variable from the universe.
	meaning co efficient between independent variable and target is zero. Please note that the co efficients derived by statsmodel in python indicates values for 
							best fit line.
	in this case distribution will look like a bell. 
	Note a flattened or squashed bell curve for density distribution indicates co relation between variables.
	In other words if the null hypothesis testing in sample data needs to be rejected the p value for any observed co relation for the respective variables 
						needs to be less than significance value.i.e 5%.
	attributes resulting in null hypothesis rejection needs to be dropped from considering into linear regression as independent variables.
assuming null hypothesis to be true what is the probability that sample drawn from the universe will result in the 
	co efficients returned? This can be found through python statsmodel package through corresponding p value for each independent variable.
Note: std error * t statistics= co efficent. this is derived by resolving t statistic formulae
	central value for tscore in this case is zero and hence t score has to be found for co efficients returned in statsmodel
	T = Xi-Xbar/Standard eror,  given X bar is 0, T*standard error=Xi

Durbin Watson test and Jarque Bera test are other statistical test for testing the quality of model.

Assumptions to be made on linear regression models:
	Will the developed linear regression mode work in production? we need to understand the assumptions being made during this type of model building.
	assumptions are about the universe and not about the sample.
	1.Population linear regression model is linear in parameters,meaning all m's in the structure are raised to be power 1.
	2.mean value of error for given set of y values for every value of x is zero. meaning distribution across positive and negative side will cancel each other 
						to make the mean value as 0.
		this means model is not suffering inefficiency in its structure. this is specification bias.
	3. whatever the value of x may be , the error will be similar across values of x higher or lower. if this is not met the model will become unreliable.
	4. homoscadasticity: Constant variation of error across different value of x. variation will be constant. if not model will give different accuracy scores 
					across different value of X.
	5. no auto correlation among errors. i.e errors of Xi and Xj are not related to each other. usually when we dont have time dimension we dont notice auto corelation among errors.
		meaning, error done at time e cannot be predicted using error doen at time (e-1)
	6. number of observations n > number of parameters estimated... this is called breadth and depth analysis. this avoids overfit model. This assumption is related to curse of dimensionality if the assumption fails.
	7. Value of X should not be constant and should have variance. also there should not be outliers.
	8. no perfect co linearity among X . if this is not the case while model will be retaining accuracy it becomes unexplainable.
	9. model is neither overfit or underfit
	10. the stochastic term(e),the error value is normally distributed.

Multicolinearity of independent variables:
	Independent variables are called so due to absence of multi colinearity expectation.
	1. structural multicolinearity occurs when new feature is created from existing feature.
	2. Data multicolinearity: This is artifact of data itself. regular corelation across variables.

	Variance inflation factor (VIF): 1/(1-r squared). when  corelation is high VIF reaches infiniti. if no corelation VIF will be close to or equal to 1.
		VIF =1 is no corelation, between 1 and 5 is moderate colinearity and >5 is severe colinearity.
		VIF can be used to estimate and test the presence of colinearity across different dimensions.

r , r squared and p values are used to validate at model level and dimension level	
Program:
If number of columns (predictors) is large but we dont have sufficient number of rows with various different permutations and combinations of values for the predictor
			 then the linear regression model will not be effective.
if one of the variable reflects values with numerical identifier that are mostly flag rather than having any quantitative value we need to replace those numeric 
					values with actual textual values followed by one hot coding columns.
In the real world create 3 datasets: training , validation and the testing . Do not use testing data until you are about to productionize which is when you do final 
			test. fine tuning in test data is not followed in production environment as it will introduce data leaks as it will introduce overfitment.
			refer data leak concept outside the course.
sometimes co efficients in linear regression would not reflect the correlation in pair plot. It is opposite of what is expected in that case. This is due to presence 
								multi colinearity in the dataset.
statsmodel is a package in python similar to R to provide statistical analysis in a deep dive manner.
regression_model.score returns r^2  (co efficient of determinant). this is not used for evaluating the models however we use adjusted r^2
adjusted R^2 = R^2 - statistical fluke/chance (i.e corelation by chance).
	statistical fluke is fix for inconsistency on distribution between sample and the universe in which sample reflects scatter plot should be perfectly 
	symetric and r value should come to zero as a result of positive and negative corelation cancelling each other. But if we draw a sample out of such universe 
	we may not get the symetric distribution but an asymetric one yielding to some r value and hence R^2 which is not a real one.
	when we add even useless dimension R^2 will approach one whereas Adjusted R^2 will control that to retain validity.

why whats offered by statsmodel is not available in sklearn? Statistical community differs on if p values are reliable or 
	not. like when some variables are removed after hypothesis testing, p value for some of the remaining variables 
	could change significantly resulting in debate of why it should change...
	meaning predictive power of some of the variables could change in the presence and absence of some variables. it could be due to multi colinearity
	presence of multi colinearity causes less reliability on the variables for some of the statisticians and hence they would not use p values.

Z scoring based scaling of data would change the co efficient ,neutralize/remove the intercept while the accuracy score remains the same  before and after. MSE would get scaled too.
	intercept becomes 0 as Z scoring will center the data around 0 and hence intercept will pass on the 0 point at the intersection of x bar and y bar graph.


Testing:
Residual vs fitted plots: will enable testing the assumption errors in if prediction is independent of value x.
	errors (residuals) are plotted against X. 
	Result interpretations:
		1. Cloud formation of distribution . No problem. finds outliers?
		2. Heteroscadisticity: Homoscadasticity is violated. finds unequal error variances? may need model restructure.
		3. Curve formation represting non linear relationship among error .detects non linearity. This may need change in the models structure.


Class notes: 
Regression is impacted by oulier, multi colinearity.
For all points that are away from best fit line, the y hat will be super imposed to the nearest point in the line . this is how algorithm works.
model tries to create multiple lines that will go through x bar and y bar but narrow down the best fit line among them.
the line that minimizes SSE (distance between y and y hat) is the best fit line. outliers will introduce larger errors and hence it needs to be treated carefully.
multi colinearity can be reduced by combining the corelated variables... like height + weight as BMI... this is called feature reengineering.
also if not the independent variables are significant in predicting Y there is a good sign of multi colinearity among X's.
we can build multiple models by keeping multi colinear X's for one and retaining the other for remaining models followed by comparing R squared.
R squared is good for Simple linear regression 

R squared = 1 - (SSR/SST)
samples are normally distributed and hence the error.  Error is not a linear function.

Assumptions : L I N E .. L for Linearity (sample beng normal and error is not), I for independence (including among errors including time series), N for normality (sample and error),  E for variance (error will have equal variance, value of X cannot be constant)

error metrics: mean absolute error (MAE), Mean sum of square error (MSE), Root mean sum of square error (RMSE). RMSE is preferred as it brings error to its own original unit/form.
regression is a parametric model..i.e by knowing weights of the X's we can predict easily. in other words these models can be expressed directly in mathematical term.
we will compare multiple models for AIC and BIC to pick the one with least values for these. also adjusted R square is important and critical which should be higher.
to tune the model remove the multi colinear variables visible from pairplot.
PCA should only be used when the dimensions are not clearly explainable and to deal with curse of dimensionality.


VIF, PCA