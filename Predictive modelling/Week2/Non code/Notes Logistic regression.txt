 x cSummary
output is binomial or multinomial. 
within multinomial we have sub categories such as output variables are ordered/unordered. 
for ordered we use ordinal LR and for unordered we use nominal LR.

error validation through log loss


Logistic regression model is a classifier. emits/projects a class for data point along with probabilities across classes.
It uses linear model to predict a class and its probabilities.


Types: binomial (binary class classification) and multinomial (Multiple class classificartion where dependent variable has multiple categories)
		multinomial could have subclasses wherein the classes would have ordering between them.
			for those multinomial without classes with go with nominal multinominal logistic regression approach (like OCR classification)

Approach
	Default: One vs Rest (OVR) . this is default for classifications using other models for ex naive bayes etc.

regularization of model is a approach of model tuning that attempts constraining a model to ensure they are generalized well and dont create a overfit model.


Building blocks:
	query point: assigns probabilities to different classes to which a query point is likely to belong.
		assigns weights and biases for the given independent variables to predict target. It uses how well a particular feature/independant variables seperates target classes clearly to assign weight
		weights can range from -infinity to +infinity. weights multipled by the respective predictor variable can be negative or positive accordingly.
		
		probability ranges between 0 (negative infinity) and 1 (positive infinity) 
		linear model cannot give probabilities but gives raw numbers which needs to be converted to probability value for which we use the mathematical transformation function called sigmoid.
		sigmoid = (1/(1+e power (-z)) where z is linear summation of weighted variables and the bias. it converts the straight line between one independent variable (x1) and Y it converts the straight line to S curve. 
			S curve goes to 0 at - infinity and 1 at +infinity. S curve meets the requirement of probability function.

		The best fit line gives the best S curve which minimizes errors. how does it happen? through under the hood learning process it minimizes the error function which is called log loss or cross entropy.
		most optimal weights will minimizes the log loss. i.e minimize the mean of log loss function computed for all the data points together.

		Note: We build models to predict what is not normal.. i.e DB server goes down occassionally which we shall predict. not every customer is a defaulter etc..
		challenge with line as a model (graph with prediction class in x axis and probability y) is it does not have lower and upper limit and the probability cannot be below 0 or above 1 in a graph that depicts probability between 0 and 1. 
			hence we do transformation of this line using sigmoid transformation using the equation given above.
			Probability is divided between below and above 0.5 to predict the class accordingly.


	Learning process:
		Odds is a ratio of something happening vs something not happening.
		probability = odds/(1+odds)   i.e odds = p/(1-p)  p=probability
		Note: beta x +c type equations = mx+c equivalent.  i.e m1x1+m2x2+...   in order e to the power (mx+c) = odds (y= class 1)
		logistic regression finds out co efficients to minimize the log loss. this results in getting log odds which is the best fit line.
		subsequently raises log odds = mx+c (beta equation) to the power of e	to get odds from which probability is derived based on equation p=odds/(1+odds)
		best fit line = log odds and then raising log odds to the power e it gets odds followed by converting odds to probability is what logistical regression model does to derive probability.

	
	assumption:
		output variable to be categorical
		attribute and log odds (function of probability) should have linear relationship to the independent variables. i.e independent variables should have linear relationship with the probabilities of its classes.
		attribute are independent of each other. 
		class of interest to be 1 and the other is 0 in binary and in multinomial LR while class of interest being code 1 rest are coded 0.
			also in case of multinomial LR , the OVR (One vs Rest) scheme creates ensemble of multiple models i.e each of the classes becoming class of interest among the rest in the round robin and end up creating multiple such models to create one single logistic regression model. This is the default approach for this technique.

		Note: Homoscadasticity, normal distribution of error terms and linear relationship between X and Y variables are not required in LR. This is because we are not doing regression and are not deriving sum of squares in LR these assumptions are held for LR.

	Validation:
		confusion matrix, accuracy, sensitivity/recall, specificity, precision.
		for confusion matrix always keep parameters in the order of actuals, predictions so that actuals will be returned in rows and prediction in columns. this is to keep consistency always.
		Accuracy: usually not reliable as class of interest is under represented in its numbers. so when the data is skewed or imbalanced depending on accuracy is risky.
		Recall : Finds the proportion of correctly predicted class of interest vs those that were missed to false negatives.
		Specificity: opposite of recall. proportion of correctly predicted class 0 vs those predictions that missed the class 0's incorrectly.
		Precision: proportion of correctly predicted class of interest to total prediction for class of interest.
		
		Note: 
		Recall and precision works against each other. so best way to get both as high values is to use good features in all the models.
		so its a business decision to go with optimal recall value vs optimal precision. for ex. in LS recall is more important considering life and death whereas in retail, precision is more important ex. identifying bad customers (defaulters) should be of high criticality.
		changes for threshold value towards above needs (threshold management) should be done outside this model. i.e sklearn.biarize to modify thresholds.


pros  
	fact that it is based on LR which has lots of assumptions lots of cons will be similar to LR. so does pros.

	based on the weights on the attributes the influence can be identified for direct or indirect relationship.
	quick to train as error function is smooth and continous and follows cross entropy and is convex.

cons
assumption of linear relationship between log odds and independent variables. 
attributes that separeates classes cleanly will end up having model stop learning. in real world we never have attributes that cleanly seperates the classes
outliers can create menace.
assumption that attributes are independent of each other which is generally not the case. in real word it complements each other.

Program
sklearn library
sometimes looking at scatter plot it gives idea on whether to use KNN or LR or decisions tree etc for classification.
in classification begin analysis with diagonal attributes to determine which one is a better predictor.