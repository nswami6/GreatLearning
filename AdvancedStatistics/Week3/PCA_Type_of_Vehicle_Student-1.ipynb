{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary modules\n",
    "#Import all the necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('D:\\Data')\n",
    "\n",
    "data_df = pd.read_csv(\"D:/GL Session/Mentor RF/vehicle.csv\")\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that all the attributes are either float or int except for the class attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple NA values with respect to each columns we will drop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop NA items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni-variate analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Univariate except class column\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4,ncols=2)\n",
    "fig.set_size_inches(12, 14)\n",
    "a = sns.distplot(data_df['compactness'] , ax=axes[0][0])\n",
    "a.set_title(\"compactness Distribution\",fontsize=15)\n",
    "a = sns.boxplot(data_df['compactness'] , orient = \"v\" , ax=axes[0][1])\n",
    "a.set_title(\"compactness Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['circularity'] , ax=axes[1][0])\n",
    "a.set_title(\"circularity Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['circularity'] , orient = \"v\" , ax=axes[1][1])\n",
    "a.set_title(\"circularity Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['distance_circularity'] , ax=axes[2][0])\n",
    "a.set_title(\"distance_circularity Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['distance_circularity'] , orient = \"v\" , ax=axes[2][1])\n",
    "a.set_title(\"distance_circularity Distribution\",fontsize=15)\n",
    "\n",
    "\n",
    "a = sns.distplot(data_df['radius_ratio'] , ax=axes[3][0])\n",
    "a.set_title(\"radius_ratio Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['radius_ratio'] , orient = \"v\" , ax=axes[3][1])\n",
    "a.set_title(\"radius_ratio Distribution\",fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Check for the next set of consecutive columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4,ncols=2)\n",
    "fig.set_size_inches(12, 14)\n",
    "a = sns.distplot(data_df['pr.axis_rectangularity'] , ax=axes[0][0])\n",
    "a.set_title(\"pr.axis_rectangularity Distribution\",fontsize=15)\n",
    "a = sns.boxplot(data_df['pr.axis_rectangularity'] , orient = \"v\" , ax=axes[0][1])\n",
    "a.set_title(\"pr.axis_rectangularity Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['max.length_rectangularity'] , ax=axes[1][0])\n",
    "a.set_title(\"max.length_rectangularity Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['max.length_rectangularity'] , orient = \"v\" , ax=axes[1][1])\n",
    "a.set_title(\"max.length_rectangularity Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['scaled_variance'] , ax=axes[2][0])\n",
    "a.set_title(\"scaled_variance Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['scaled_variance'] , orient = \"v\" , ax=axes[2][1])\n",
    "a.set_title(\"scaled_variance Distribution\",fontsize=15)\n",
    "\n",
    "\n",
    "a = sns.distplot(data_df['scaled_variance.1'] , ax=axes[3][0])\n",
    "a.set_title(\"scaled_variance.1 Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['scaled_variance.1'] , orient = \"v\" , ax=axes[3][1])\n",
    "a.set_title(\"scaled_variance.1 Distribution\",fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=6,ncols=2)\n",
    "fig.set_size_inches(20, 18)\n",
    "a = sns.distplot(data_df['scaled_radius_of_gyration'] , ax=axes[0][0])\n",
    "a.set_title(\"scaled_radius_of_gyration Distribution\",fontsize=15)\n",
    "a = sns.boxplot(data_df['scaled_radius_of_gyration'] , orient = \"v\" , ax=axes[0][1])\n",
    "a.set_title(\"scaled_radius_of_gyration Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['scaled_radius_of_gyration.1'] , ax=axes[1][0])\n",
    "a.set_title(\"scaled_radius_of_gyration.1 Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['scaled_radius_of_gyration.1'] , orient = \"v\" , ax=axes[1][1])\n",
    "a.set_title(\"scaled_radius_of_gyration.1 Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['skewness_about'] , ax=axes[2][0])\n",
    "a.set_title(\"skewness_about Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['skewness_about'] , orient = \"v\" , ax=axes[2][1])\n",
    "a.set_title(\"skewness_about Distribution\",fontsize=15)\n",
    "\n",
    "\n",
    "a = sns.distplot(data_df['skewness_about.1'] , ax=axes[3][0])\n",
    "a.set_title(\"skewness_about.1 Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['skewness_about.1'] , orient = \"v\" , ax=axes[3][1])\n",
    "a.set_title(\"skewness_about.1 Distribution\",fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "a = sns.distplot(data_df['skewness_about.2'] , ax=axes[4][0])\n",
    "a.set_title(\"skewness_about.2 Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['skewness_about.2'] , orient = \"v\" , ax=axes[4][1])\n",
    "a.set_title(\"skewness_about.2 Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(data_df['hollows_ratio'] , ax=axes[5][0])\n",
    "a.set_title(\"hollows_ratio Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.boxplot(data_df['hollows_ratio'] , orient = \"v\" , ax=axes[5][1])\n",
    "a.set_title(\"hollows_ratio Distribution\",fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the plots that the following variables have outliers - radius_ratio,pr.axis_aspect_ratio,max.length_aspect_ratio,scaled_variance,scaled_variance.1,scaled_radius_of_gyration.1,skewness_about,skewness_about.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi- Variate Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(data_df.corr(), annot=True)  # plot the correlation coefficients as a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many columns are co-related to each other or in other words the correlation exists for many columns and the highest is between max-length_rectangularity and circularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Point to be noted :  Class would be the target variable. Should be removed when PCA is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check for pair plots\n",
    "sns.pairplot(data_df,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(25,30))\n",
    "data_df.boxplot(figsize=(25,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we observe that the same columns have outliers, let us try to handle them by using custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment :  define a custom function- If for a particular column the max value is greater than that assign max value,same logic for min value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(col):\n",
    "    sorted(col)\n",
    "    Q1,Q3=np.percentile(col,[25,75])\n",
    "    IQR=Q3-Q1\n",
    "    lower_range= Q1-(1.5 * IQR)\n",
    "    upper_range= Q3+(1.5 * IQR)\n",
    "    return lower_range, upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lratio,uratio=remove_outlier(data_df['radius_ratio'])\n",
    "data_df['radius_ratio']=np.where(data_df['radius_ratio']>uratio,uratio,data_df['radius_ratio'])\n",
    "data_df['radius_ratio']=np.where(data_df['radius_ratio']<lratio,lratio,data_df['radius_ratio'])\n",
    "\n",
    "lraxis,uraxis=remove_outlier(data_df['pr.axis_aspect_ratio'])\n",
    "data_df['pr.axis_aspect_ratio']=np.where(data_df['pr.axis_aspect_ratio']>uraxis,uraxis,data_df['pr.axis_aspect_ratio'])\n",
    "data_df['pr.axis_aspect_ratio']=np.where(data_df['pr.axis_aspect_ratio']<lraxis,lraxis,data_df['pr.axis_aspect_ratio'])\n",
    "\n",
    "lraspect,uraspect=remove_outlier(data_df['max.length_aspect_ratio'])\n",
    "data_df['max.length_aspect_ratio']=np.where(data_df['max.length_aspect_ratio']>uraspect,uraspect,data_df['max.length_aspect_ratio'])\n",
    "data_df['max.length_aspect_ratio']=np.where(data_df['max.length_aspect_ratio']<lraspect,lraspect,data_df['max.length_aspect_ratio'])\n",
    "\n",
    "lrscaled_var,urscaled_var=remove_outlier(data_df['scaled_variance'])\n",
    "data_df['scaled_variance']=np.where(data_df['scaled_variance']>urscaled_var,urscaled_var,data_df['scaled_variance'])\n",
    "data_df['scaled_variance']=np.where(data_df['scaled_variance']<lrscaled_var,lrscaled_var,data_df['scaled_variance'])\n",
    "\n",
    "lrscal_Var1,urscal_Var1=remove_outlier(data_df['scaled_variance.1'])\n",
    "data_df['scaled_variance.1']=np.where(data_df['scaled_variance.1']>urscal_Var1,urscal_Var1,data_df['scaled_variance.1'])\n",
    "data_df['scaled_variance.1']=np.where(data_df['scaled_variance.1']<lrscal_Var1,lrscal_Var1,data_df['scaled_variance.1'])\n",
    "\n",
    "lradius_gyration,uradius_gyration=remove_outlier(data_df['scaled_radius_of_gyration.1'])\n",
    "data_df['scaled_radius_of_gyration.1']=np.where(data_df['scaled_radius_of_gyration.1']>uradius_gyration,uradius_gyration,data_df['scaled_radius_of_gyration.1'])\n",
    "data_df['scaled_radius_of_gyration.1']=np.where(data_df['scaled_radius_of_gyration.1']<lradius_gyration,lradius_gyration,data_df['scaled_radius_of_gyration.1'])\n",
    "\n",
    "\n",
    "lSkew,uSkew=remove_outlier(data_df['skewness_about'])\n",
    "data_df['skewness_about']=np.where(data_df['skewness_about']>uSkew,uSkew,data_df['skewness_about'])\n",
    "data_df['skewness_about']=np.where(data_df['skewness_about']<lSkew,lSkew,data_df['skewness_about'])\n",
    "\n",
    "lSkew1,uSkew1=remove_outlier(data_df['skewness_about.1'])\n",
    "data_df['skewness_about.1']=np.where(data_df['skewness_about.1']>uSkew1,uSkew1,data_df['skewness_about.1'])\n",
    "data_df['skewness_about.1']=np.where(data_df['skewness_about.1']<lSkew1,lSkew1,data_df['skewness_about.1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for outliers after handling them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(25,30))\n",
    "data_df.boxplot(figsize=(25,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Class column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Names column has unique values , so will remove it from the dataset.More over there is no point in adding ID for PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardising before processing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All variables must be on same scale, hence we can omit scaling.\n",
    "# Standardization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_new.boxplot(figsize=(20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a covariance matrix for identifying Principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# Step 1 - Create covariance matrix\n",
    "\n",
    "cov_matrix = np.cov(data_new.T)\n",
    "print('Covariance Matrix \\n%s', cov_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even if we take the transpose of the covariance matrix it results in same value as that of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Correlation and Covarince Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now without Scaling lets check out correlation matrix\n",
    "df_corr = data_df.copy()\n",
    "df_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With standardisation (Without standardisation also, correlation matrix yields same result)\n",
    "data_new.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " “Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables.Correlation is a function of the covariance. \n",
    " You can obtain the correlation coefficient of two variables by dividing the covariance of these variables by the product of the standard deviations of the same values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  state that above three approaches yield the same eigenvectors and eigenvalue pairs:\n",
    "\n",
    "1.Eigen decomposition of the covariance matrix after standardizing the data.\n",
    "\n",
    "2.Eigen decomposition of the correlation matrix.\n",
    "\n",
    "3.Eigen decomposition of the correlation matrix after standardizing the data.\n",
    "\n",
    "Finally we can say that after scaling - the covariance and the correlation have the same values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify eigen values and eigen vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2- Get eigen values and eigen vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are totally 4 eigen values greater than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cummulative Distribution of Eigen values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually we can observe that their is steep drop in variance explained with increase in number of PC's.\n",
    "# We will proceed with 4 components here. But depending on requirement 90% variation or 5 components will also do good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ploting \n",
    "plt.figure(figsize=(10 , 5))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use PCA command from sklearn and find Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit learn PCA here. It does all the above steps and maps data to PCA dimensions in one shot\n",
    "\n",
    "\n",
    "# NOTE - we are generating only 4 PCA dimensions (dimensionality reduction from 18 to 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "var #cumulative sum of variance explained with [n] features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cumulative % gives the percentage of variance accounted for by the n components. For example, the cumulative percentage for the second component is the sum of the percentage of variance for the first and second components. It helps in deciding the number of components by selecting the components which explained the high variance\n",
    "\n",
    "In the above array we see that the first feature explains 54.1% of the variance within our data set while the first two explain 72.2 and so on. If we employ 4 features we capture ~ 85% of the variance within the dataset, thus we gain very little by implementing an additional feature (think of this as diminishing marginal return on total variance explained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between components and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This heatmap and the color bar basically represent the correlation between the various feature \n",
    "# and the principal component itself\n",
    "# Component 2 looks more related to aspect  - We can label it as aspect property\n",
    "# Depending on relations ship, we could go ahead and label relationship with features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a statistical technique and uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables. PCA also is a tool to reduce multidimensional data to lower dimensions while retaining most of the information. Principal Component Analysis (PCA) is a well-established mathematical technique for reducing the dimensionality of data, while keeping as much variation as possible.\n",
    "\n",
    "This PCA can only be done on continous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
